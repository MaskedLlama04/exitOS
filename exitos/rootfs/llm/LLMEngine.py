import json
import traceback
import requests
import os
from bottle import template, request, response, request as bottle_request

# Global logger from parent (will be set in init_routes or imported if available)
logger = None

class LLMEngine:
    """
    Class to handle communication with Ollama LLM with conversation history and tools.
    """
    def __init__(self, model=None, ollama_url=None):
        # Ollama Configuration (Hardcoded perque vull)
        self.model = "llama3.1:latest"
        self.ollama_base_url = "http://192.168.191.70:11434"
        
        self.api_url = f"{self.ollama_base_url}/api/chat"
        self.headers = {"Content-Type": "application/json"}

        self.system_prompt = (
            "Ets un expert en gesti√≥ energ√®tica de la plataforma eXiT. "
            "La teva missi√≥ √©s ajudar l'usuari a entendre la seva configuraci√≥ d'autoconsum, "
            "optimitzaci√≥ de bateries i generaci√≥ solar. Respon de manera amable, clara i professional, "
            "preferiblement en catal√†. Si l'usuari no coneix el tema, explica els conceptes de manera senzilla.\n\n"
            "Quan l'usuari et demani una recomanaci√≥ de configuraci√≥ d'optimitzaci√≥:\n"
            "1. Utilitza l'eina 'get_available_device_types' per veure quins tipus de dispositius existeixen "
            "i quins par√†metres cal configurar per a cadascun.\n"
            "2. Opcionalment, utilitza 'get_optimization_configs' per veure les configuracions actuals de l'usuari.\n"
            "3. Basant-te en la situaci√≥ que t'explica l'usuari (tipus de dispositiu que t√©, necessitats energ√®tiques, etc.), "
            "recomana quin tipus de dispositiu escollir i quins valors posar a cada par√†metre.\n"
            "4. Explica SEMPRE el raonament darrere de cada decisi√≥: per qu√® has escollit aquell tipus, "
            "per qu√® proposes aquells valors concrets per a les restriccions, i com afectar√† a l'optimitzaci√≥ energ√®tica.\n"
            "5. Si no tens prou informaci√≥ per fer una recomanaci√≥ precisa, pregunta a l'usuari "
            "les dades que necessites (capacitat del dispositiu, consum habitual, etc.)."
        )
        self.conversations = {}
        self.tools = {}

        if logger:
            logger.info(f"üîß LLMEngine inicialitzat (OLLAMA):")
            logger.info(f"   - Model: {self.model}")
            logger.info(f"   - API URL: {self.api_url}")

    def register_tool(self, name, func, description, parameters):
        """
        Registra una nova eina que l'LLM pot utilitzar.
        """
        tool_definition = {
            "type": "function",
            "function": {
                "name": name,
                "description": description,
                "parameters": parameters
            }
        }
        self.tools[name] = {
            "definition": tool_definition,
            "func": func
        }
        if logger:
            logger.info(f"üõ†Ô∏è Eina registrada: {name}")

    def get_response(self, user_input, session_id="default"):
        """
        Obt√© resposta de l'Ollama executant eines si cal.
        """
        try:
            # Inicialitzar conversa
            if session_id not in self.conversations:
                self.conversations[session_id] = [
                    {"role": "system", "content": self.system_prompt}
                ]
            
            # Afegir missatge usuari
            self.conversations[session_id].append({
                "role": "user", 
                "content": user_input
            })
            
            # Bucle d'execuci√≥ d'eines (m√†x 5 iteracions)
            for _ in range(5):
                available_tools = [t["definition"] for t in self.tools.values()] if self.tools else None
                
                payload = {
                    "model": self.model,
                    "messages": self.conversations[session_id],
                    "stream": False
                }
                
                # Afegir eines
                if available_tools:
                    payload["tools"] = available_tools

                if logger:
                    logger.info(f"ü§ñ Enviant petici√≥ a OLLAMA...")

                res = requests.post(self.api_url, headers=self.headers, json=payload, timeout=120)
                
                if res.status_code != 200:
                    error_msg = f"Error Ollama {res.status_code}: {res.text}"
                    if logger: logger.error(f"‚ùå {error_msg}")
                    return f"‚ùå {error_msg}"

                data = res.json()
                
                # Ollama endpoint /api/chat returns message at root
                response_message = data.get("message", {})

                content = response_message.get("content", "")
                tool_calls = response_message.get("tool_calls", [])
                
                # Afegim la resposta de l'assistent a l'historial
                self.conversations[session_id].append(response_message)
                
                # Si no hi ha eines, hem acabat
                if not tool_calls:
                    if logger: logger.info(f"‚úÖ Resposta final: {content[:50]}...")
                    return content
                
                if logger: logger.info(f"üõ†Ô∏è Executant {len(tool_calls)} eines...")
                
                # Executar eines
                for tool_call in tool_calls:
                    fn_name = tool_call["function"]["name"]
                    fn_args = tool_call["function"]["arguments"]
                    
                    if fn_name in self.tools:
                        if logger: logger.info(f"   ‚ñ∂Ô∏è {fn_name}({fn_args})")
                        try:
                            result = self.tools[fn_name]["func"](**fn_args)
                            result_str = str(result)
                        except Exception as e:
                            result_str = f"Error: {e}"
                            if logger: logger.error(f"   ‚ùå Error: {e}")
                        
                        tool_msg = {
                            "role": "tool",
                            "content": result_str,
                            "name": fn_name
                        }
                        
                        self.conversations[session_id].append(tool_msg)

                    else:
                        self.conversations[session_id].append({
                            "role": "tool",
                            "content": f"Error: Tool {fn_name} not found",
                            "name": fn_name
                        })

            return "‚ö†Ô∏è L√≠mit d'iteracions d'eines superat."

        except requests.exceptions.ConnectionError:
            return f"‚ùå Error de connexi√≥ amb Ollama. Verifica la URL."
        except Exception as e:
            if logger: logger.error(f"‚ùå Error inesperat: {e}\n{traceback.format_exc()}")
            return f"Error inesperat: {e}"
    
    def clear_conversation(self, session_id="default"):
        if session_id in self.conversations:
            self.conversations[session_id] = [
                {"role": "system", "content": self.system_prompt}
            ]
            return True
        return False


# Inst√†ncia global
llm_engine = LLMEngine()


def _add_cors_headers():
    """Afegeix headers CORS a la resposta."""
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type'


def init_routes(app, external_logger):
    global logger
    logger = external_logger

    # Hook global CORS: s'executa despr√©s de cada request
    @app.hook('after_request')
    def enable_cors():
        _add_cors_headers()

    @app.route('/llmChat')
    def llm_chat_page():
        return template('./www/llmChat.html')

    @app.route('/llm_response', method=['POST', 'OPTIONS'])
    def llm_response():
        # Resposta r√†pida al preflight CORS
        if request.method == 'OPTIONS':
            return {}

        if logger:
            logger.info("üîµ Endpoint /llm_response cridat")
        try:
            data = request.json
            if logger:
                logger.info(f"   - Dades rebudes: {data}")
            
            if not data:
                response.status = 400
                return json.dumps({'status': 'error', 'message': 'Dades buides'})
            
            user_message = data.get('message', '')
            if not user_message:
                return json.dumps({'status': 'error', 'message': 'El missatge est√† buit'})
            
            session_id = bottle_request.environ.get('REMOTE_ADDR', 'default')
            
            response_text = llm_engine.get_response(user_message, session_id)
            
            response.content_type = 'application/json'
            return json.dumps({
                'status': 'ok',
                'response': response_text
            })
            
        except Exception as e:
            if logger:
                logger.error(f"‚ùå Error en LLM response: {e}")
                logger.error(traceback.format_exc())
            return json.dumps({
                'status': 'error', 
                'message': 'Ho sento, hi ha hagut un error sol¬∑licitant la resposta.'
            })

    @app.route('/llm_clear', method=['POST', 'OPTIONS'])
    def llm_clear():
        """
        Endpoint per esborrar l'historial de conversa.
        """
        if request.method == 'OPTIONS':
            return {}
        try:
            session_id = bottle_request.environ.get('REMOTE_ADDR', 'default')
            llm_engine.clear_conversation(session_id)
            return json.dumps({'status': 'ok', 'message': 'Conversa esborrada'})
        except Exception as e:
            if logger: logger.error(f" Error esborrant conversa: {e}")
            return json.dumps({'status': 'error', 'message': 'Error esborrant conversa'})

    @app.route('/llm_history', method=['GET', 'OPTIONS'])
    def llm_history():
        """
        Retorna l'historial de missatges de la sessi√≥ actual (sense missatges de sistema ni d'eines).
        """
        if request.method == 'OPTIONS':
            return {}
        try:
            session_id = bottle_request.environ.get('REMOTE_ADDR', 'default')
            conversation = llm_engine.conversations.get(session_id, [])

            # Filtrem: nom√©s retornem missatges de l'usuari i l'assistent, no system ni tool
            visible_messages = [
                {"role": msg["role"], "content": msg.get("content", "")}
                for msg in conversation
                if msg["role"] in ("user", "assistant") and msg.get("content", "").strip()
            ]

            response.content_type = 'application/json'
            return json.dumps({'status': 'ok', 'messages': visible_messages})
        except Exception as e:
            if logger: logger.error(f"Error recuperant historial: {e}")
            return json.dumps({'status': 'error', 'messages': []})


    @app.route('/llm_test', method='GET')
    def llm_test():
        if logger:
            logger.info("üß™ Test endpoint cridat")
        return json.dumps({
            'status': 'ok',
            'message': 'LLM routes are working!',
            'ollama_url': llm_engine.ollama_base_url,
            'model': llm_engine.model
        })
    
    if logger:
        logger.info("‚úÖ Rutes LLM registrades:")
        logger.info("   - GET  /llmChat")
        logger.info("   - POST /llm_response")
        logger.info("   - POST /llm_clear")
        logger.info("   - GET  /llm_test")
